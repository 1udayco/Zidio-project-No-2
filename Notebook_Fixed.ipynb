{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed59890",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Fixed Version\n",
    "This notebook contains the corrected implementation for customer churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Read the datasets\n",
    "train_df = pd.read_csv(\"customer_churn_dataset-training-master.csv\")\n",
    "test_df = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
    "\n",
    "print(\"Train Data Shape:\", train_df.shape)\n",
    "print(\"Test Data Shape:\", test_df.shape)\n",
    "print(\"\\nTrain Data Info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "print(\"\\nChurn distribution in training data:\")\n",
    "print(train_df['Churn'].value_counts())\n",
    "print(\"\\nChurn distribution in test data:\")\n",
    "print(test_df['Churn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1db231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Preprocess the training and test data\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying original data\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    \n",
    "    # Encode categorical features\n",
    "    cat_cols = ['Gender', 'Subscription Type', 'Contract Length']\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on training data and transform both\n",
    "        train_processed[col] = le.fit_transform(train_processed[col])\n",
    "        test_processed[col] = le.transform(test_processed[col])\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Drop CustomerID column\n",
    "    train_processed.drop('CustomerID', axis=1, inplace=True)\n",
    "    test_processed.drop('CustomerID', axis=1, inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    X_train = train_processed.drop('Churn', axis=1)\n",
    "    y_train = train_processed['Churn']\n",
    "    X_test = test_processed.drop('Churn', axis=1)\n",
    "    y_test = test_processed['Churn']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, label_encoders\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, y_train, X_test, y_test, label_encoders = preprocess_data(train_df, test_df)\n",
    "\n",
    "print(\"Processed Train Features Shape:\", X_train.shape)\n",
    "print(\"Processed Test Features Shape:\", X_test.shape)\n",
    "print(\"\\nFeature columns:\")\n",
    "print(X_train.columns.tolist())\n",
    "print(\"\\nFirst few processed rows:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imputation_scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and scale features\n",
    "# Initialize imputer and scaler\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# Transform test data using fitted transformers\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "print(\"Data preprocessing completed!\")\n",
    "print(f\"Training data shape after preprocessing: {X_train_scaled.shape}\")\n",
    "print(f\"Test data shape after preprocessing: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf957cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Model coefficients shape: {model.coef_.shape}\")\n",
    "print(f\"Model intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=== Model Performance ===\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n=== Classification Report (Test Set) ===\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (Test Set) ===\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_names = X_train.columns.tolist()\n",
    "feature_importance = abs(model.coef_[0])\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=== Feature Importance ===\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance (Absolute Coefficient Value)')\n",
    "    plt.title('Feature Importance in Churn Prediction')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "# Create 'model' directory if it doesn't exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Save the models\n",
    "pickle.dump(imputer, open(\"model/imputer.pkl\", \"wb\"))\n",
    "pickle.dump(scaler, open(\"model/scaler.pkl\", \"wb\"))\n",
    "pickle.dump(model, open(\"model/churn_model.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoders, open(\"model/label_encoders.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Models saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- model/imputer.pkl\")\n",
    "print(\"- model/scaler.pkl\")\n",
    "print(\"- model/churn_model.pkl\")\n",
    "print(\"- model/label_encoders.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction with sample data\n",
    "def test_prediction():\n",
    "    \"\"\"\n",
    "    Test the saved models with sample data\n",
    "    \"\"\"\n",
    "    # Load the saved models\n",
    "    loaded_imputer = pickle.load(open(\"model/imputer.pkl\", \"rb\"))\n",
    "    loaded_scaler = pickle.load(open(\"model/scaler.pkl\", \"rb\"))\n",
    "    loaded_model = pickle.load(open(\"model/churn_model.pkl\", \"rb\"))\n",
    "    loaded_encoders = pickle.load(open(\"model/label_encoders.pkl\", \"rb\"))\n",
    "    \n",
    "    # Sample customer data\n",
    "    sample_data = {\n",
    "        'Age': 35,\n",
    "        'Gender': 'Male',\n",
    "        'Tenure': 24,\n",
    "        'Usage Frequency': 15,\n",
    "        'Support Calls': 3,\n",
    "        'Payment Delay': 5,\n",
    "        'Subscription Type': 'Premium',\n",
    "        'Contract Length': 'Annual',\n",
    "        'Total Spend': 750.0,\n",
    "        'Last Interaction': 7\n",
    "    }\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    gender_encoded = loaded_encoders['Gender'].transform([sample_data['Gender']])[0]\n",
    "    sub_type_encoded = loaded_encoders['Subscription Type'].transform([sample_data['Subscription Type']])[0]\n",
    "    contract_encoded = loaded_encoders['Contract Length'].transform([sample_data['Contract Length']])[0]\n",
    "    \n",
    "    # Create feature array\n",
    "    features = np.array([[\n",
    "        sample_data['Age'],\n",
    "        gender_encoded,\n",
    "        sample_data['Tenure'],\n",
    "        sample_data['Usage Frequency'],\n",
    "        sample_data['Support Calls'],\n",
    "        sample_data['Payment Delay'],\n",
    "        sub_type_encoded,\n",
    "        contract_encoded,\n",
    "        sample_data['Total Spend'],\n",
    "        sample_data['Last Interaction']\n",
    "    ]])\n",
    "    \n",
    "    # Preprocess\n",
    "    features = loaded_imputer.transform(features)\n",
    "    features = loaded_scaler.transform(features)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = loaded_model.predict(features)[0]\n",
    "    probability = loaded_model.predict_proba(features)[0]\n",
    "    \n",
    "    print(\"=== Sample Prediction Test ===\")\n",
    "    print(f\"Sample customer data: {sample_data}\")\n",
    "    print(f\"Prediction: {'Churn' if prediction == 1 else 'No Churn'}\")\n",
    "    print(f\"Churn Probability: {probability[1]:.4f}\")\n",
    "    print(f\"No Churn Probability: {probability[0]:.4f}\")\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Run the test\n",
    "test_prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}